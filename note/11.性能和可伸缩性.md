# 11. 性能可可伸缩性
---

- 使用线程最主要的原因是提高性能。使用线程可以使程序更加充分的发挥出闲置的处理能力，从而更好的利用资源；并能够使程序在现有任务正在运行的情况下立刻开始着手处理新的任务，从而提高系统的响应性。

## 11.1 性能的思考
- 改进性能意味着用更少的资源做更多的事情。
- 资源包括CPU周期、内存、网络带宽、I/O带宽、数据库请求、磁盘空间、以及其他一些资源。
- 当活动的运行因某个特定资源受阻时，我们称之为受限于该资源：受限于CPU，受限于数据库。
- 虽然目标是提高性能，但是使用多线程总会引入一些性能开销：与协调线程相关的开销(加锁、信号、内存同步)，增加的上下文切换，线程的创建和消亡，以及调度的开销。当线程被过度使用后，这些开销会超过吞吐量响应性和计算能力带来的补偿。
- 另一方面一个没有经过良好并发设计的应用程序，甚至比相同功能的顺序的程序性能更差。
- 为了实现更好的性能，我们需要更有效的利用现有的处理资源，使CPU尽可能处于忙碌状态。

### 11.1.1 性能“遭遇”可伸缩性
- 应用程序可以从很多个角度来衡量：
	- 服务时间、等待时间、(衡量有多快)
	- 吞吐量、生产量、可伸缩性、(衡量有多少)
	- 效率、

>可伸缩性指的是：当增加计算资源的时候(比如增加额外的CPU数量、内存、存储器、I/O带宽)，吞吐量和生产量能够得到相应的改进。

- 传统调优用最小代价完成相同工作，比如通过缓存来重用以前计算的结果。为可伸缩性调优时，需要用更多的资源做更多的事情。
- 在Server应用程序中，“有多少”往往比“有多快”受到更多的关注。

### 11.1.2 对性能的权衡进行评估
- 大多数性能的决定需要多个变量，并且高度依赖于发生的环境。在决定某个方案比其他方案“更快”之前，先问你自己一些问题：
	- 你所谓的更快指的是什么
	- 在什么样的条件下你的方案能够真正运行的更快？在轻负载还是重负载下？大数据集还是小数据集？是否支持你的测量标准的答案？
	- 这些条件在你的环境中发生的频率？是否支持你的测量标准的答案？
	- 这些代码在其他环境的不同条件下被用到的可能性？
	- 你用什么样隐含的代价，比如增加的开发风险或维护性，换取了性能的提高？这个权衡的决定是否正确？
- 测评。不要臆测。
- 免费的perfbar应用程序可以给你一张相当不错的图表，告诉你CPU究竟是如何忙碌地工作，并且你的目标通常是保持CPU的忙碌，这便是一个很好的方式，使你能够评估你是否需要性能调节，或者你调节的效果如何。

### 11.2 Amdahl定律
- 大多数并发程序由一系列并行和串行化的片段组成。Amdahl定律描述了在一个系统中，基于可并行化和串行化的组件各自所占的比重，程序通过获得额外的计算资源，理论上能够加速多少。如果F是必须串行化执行的比重，那么Amdahl定律告诉我们，在一个N处理器的机器中，我们最多可以加速：

　　　　　　　　![Amdahl定律](http://i.imgur.com/IGU0OCH.png)


　　　　　　![不同串行化的百分比](http://i.imgur.com/3GpNBa1.png)

> 所有的并发程序都有一些串行源。

### 11.2.1 定性地应用Amdahl定律
- 当我们评估一个算法的时候，考虑其在成百甚至上千个处理器的情况下受到的限制，能够帮助我们洞察伸缩性的极限的出现。例如，用来减小锁的粒度：分拆锁(把一个锁分拆成两个)，分离锁(把一个锁分拆成多个锁)。透过Amdahl定律来审视它们，我们发现把一个锁分拆成两个，看上去没能在利用多处理器上帮助我们很多，但是分离锁的效果却很好，因为分离出的数量可随着处理器数量的增加而增长。

## 11.3 线程引入的开销
- 单线程程序即不存在调度问题，也不存在同步的开销，不需要使用锁来保证数据结构的一致性。调度和线程内部的协调都要付出性能的开销；对于性能改进的线程来说，并行带来的性能优势必须超过并发锁引入的开销。

### 11.3.1 切换上下文
- 如果主线程是唯一可调度的线程，它绝不会排除在调度之外。从另一方面看，如果可运行的线程数大于CPU的数量，那么OS最终会强行换出正在执行的线程，从而使其他线程能够使用CPU。这会引起上下文切换，它会保存当前运行线程的执行上下文，并重建新调入线程的执行上下文。
- 切换上下文是要付出代价的；线程的调度需要操控OS和JVM中共享的数据结构。你的程序与OS、JVM使用相同的CPU；CPU在JVM和OS的代码花费越多时间，意味着用于你的程序的时间就越少。但是JVM和OS活动的花费并不是切换上下文开销的唯一来源。当一个新的线程被换入后，它所需要的数据可能不在当前处理器本地的缓存中，所以切换上下文会引起缓存缺失的小恐慌，因此线程在第一次调度的时候会运行的稍慢一些。即使有很多其他正在等待的线程，调度程序也会为每一个可运行的线程分配一个最小执行时间的定额。就是因为这个原因：它分期偿付切换上下文的开销，获得更多不中断的执行时间，从整体上提高了吞吐量(以损失响应性为代价)。
- 当线程因为竞争一个锁而被阻塞时，JVM通常会将这个线程挂起，允许它被换出。如果线程频繁发生阻塞，那线程就不能完整使用它的调度限额了。一个程序发生越多的阻塞(阻塞I/O，等待竞争锁，或者等待条件变量)，与受限于CPU的程序相比，就会造成越多的上下文切换，这增加了调度的开销，并减少了吞吐量。
- 切换上下文真正的开销根据不同的平台而变化，但是一条好的经验性原则是：在大多数同用的处理器中，上下文切换的开销相当于5000到10000个时钟周期，或者几微秒。
- Unix系统的vmstat命令和Windows系统的perfmon工具都能报告上下文切换次数和内核占用的时间等信息。高内核占用率(超过10%)通常象征繁重的调度活动，这很可能是由I/O阻塞，或竞争锁引起的。

### 11.3.2 内存同步
- 性能的开销有几个来源。synchronized和volatile提供的可见性保证要求使用一个特殊的、名为存储关卡(memory barrier)的指令，来刷新缓存，使缓存无效，刷新硬件的写缓冲，并延迟执行的传递。存储关卡可能同样会对性能产生影响，因为它们抑制了其他编译器的优化；在存储关卡中，大多数操作是不能被重排序的。
- 现代的JVM能够通过优化，解除经确证不存在竞争的锁，从而减少额外的同步。
- 更加成熟的JVM可以使用**逸出分析**来识别本地对象的引用并没有在堆中暴露，并且因此成为线程本地的。
- 以下代码的getStoogeNames中，对List仅有的引用是本地变量stooges，栈限制的变量自动默认为线程本地的。在本地执行getStoogeNames，至少需要获取/释放Vector的锁4次，每个add一次，toString一次，然而，一个聪明运行时编译器，能够合并这些调用，然后发现stooges和它的内部状态一直没有逸出，因此这4次对锁的请求就可以被消除了。

		//锁省略的候选程序
		 public final class ThreeStooges {
		    private final Set<String> stooges = new HashSet<String>();
		
		    public ThreeStooges() {
		        stooges.add("Moe");
		        stooges.add("Larry");
		        stooges.add("Curly");
		    }
		
		    public boolean isStooge(String name) {
		        return stooges.contains(name);
		    }
		
		    public String getStoogeNames() {
		        List<String> stooges = new Vector<String>();
		        stooges.add("Moe");
		        stooges.add("Larry");
		        stooges.add("Curly");
		        return stooges.toString();
		    }
		}

- 即使没有逸出分析，编译器同样可以进行**锁的粗化**，把邻近的synchronized块用相同的锁合并起来。在getStoogeNames中，JVM如果使用锁的粗化，可能会把3个add调用结合起来，并对toString使用单独的锁请求和释放，在synchronized块的内部，利用启发式方法产生同步开销，而不是指令式方法。这不仅仅减少了同步的开销，同时也给予优化者更大的代码块，很可能成就了进一步的优化。
- 一个线程中的同步也可能影响到其他线程的性能。同步造成了共享内存总线上的通信量；这个总线的带宽是有限的，所有进程都共享这条总线。如果线程必须竞争同步带宽，所有使用到同步的线程都会受阻。

### 11.3.3 阻塞
- 非竞争的同步可以由JVM完全掌控，而竞争的同步可能需要OS的活动，这会增大开销。当锁为竞争性的时候，失败的线程必然发生阻塞。JVM既能**自旋等待**，或者在操作系统中挂起这个被阻塞的线程。
- 自旋等待适合短期的等待，挂起适合长期的等待。
- 阻塞归因于锁的竞争，线程持有这样的锁：当它释放该锁的时候，它必须通知OS，重新开始因该锁而阻塞的线程。

## 11.4 减少锁的竞争
- 串行化会损害可伸缩性，上下文切换会损害性能。竞争性的锁会同时导致这两种损失，所以减少锁的竞争能够改进性能和可伸缩性。
- 访问独占锁守护的资源是串行的——一次只能有一个线程访问它。当然，我们有更好的理由使用锁，比如避免数据过期，但是这样的安全性是用很大的代价换来的。对锁长期的竞争会限制可伸缩性。

>并发程序中，对可伸缩性首要的威胁是独占的资源锁

- 有两个原因影响着锁的竞争性：
	- 锁被请求的频率
	- 每次持有该锁的时间
- 两者乘积足够小，大多数请求锁的尝试是非竞争的，但是请求量特别大，线程将会阻塞以等待锁；在极端的情况下，处理器将会闲置，即使仍有大量的工作等着完成。
- 有3中方式减少锁的竞争：
	- 减少持有锁的时间；
	- 减少请求锁的频率
	- 或者用协调机制取代独占锁，从而允许更强的并发性。

### 11.4.1 缩小锁的范围(“快进快出”)
- 减少竞争发生可能性的有效方式是尽可能缩短把持锁的时间。这可以通过把与锁无关的代码移出synchronized块来实现，尤其是那些花费昂贵的操作，以及那些潜在的阻塞操作，比如I/O操作。

		//持有锁超过必要的时间
		@ThreadSafe
		public class AttributeStore {
		    @GuardedBy("this") private final Map<String, String>
		            attributes = new HashMap<String, String>();
		
		    public synchronized boolean userLocationMatches(String name,
		                                                    String regexp) {
		        String key = "users." + name + ".location";
		        String location = attributes.get(key);
		        if (location == null)
		            return false;
		        else
		            return Pattern.matches(regexp, location);
		    }
		}

- 以下代码减少锁持续的时间

		@ThreadSafe
		public class BetterAttributeStore {
		    @GuardedBy("this") private final Map<String, String>
		            attributes = new HashMap<String, String>();
		
		    public boolean userLocationMatches(String name, String regexp) {
		        String key = "users." + name + ".location";
		        String location;
		        synchronized (this) {
		            location = attributes.get(key);
		        }
		        if (location == null)
		            return false;
		        else
		            return Pattern.matches(regexp, location);
		    }
		}

- 缩小userLocationMatches方法中锁守护的范围，大大减少了调用中遇到锁住情况的次数。由Amdahl定律得知，这消除了可伸缩性的一个阻碍，因为串行化的代码少了。
- 因为AttributeStore只有一个状态变量，attributes，我们可用**代理线程安全**的技术。通过使用线程安全的Map来取代。

### 11.4.2 减小锁的粒度
- 减小持有锁的时间比例的另一种方式是让线程减少调用它的频率。
- 这可用通过**分拆锁(lock splitting)**和**分离锁(lock striping)**来实现，也就是采用相互独立的锁，守卫多个独立的状态变量，在改变之前，它们都是由一个锁守护的。这些技术减小了锁发生时的粒度，潜在实现了更好的可伸缩性——但是使用更多的锁同样会增加死锁的风险。
- 应当分拆锁的候选程序

		@ThreadSafe
		public class ServerStatusBeforeSplit {
		    @GuardedBy("this") public final Set<String> users;
		    @GuardedBy("this") public final Set<String> queries;
		
		    public ServerStatusBeforeSplit() {
		        users = new HashSet<String>();
		        queries = new HashSet<String>();
		    }
		
		    public synchronized void addUser(String u) {
		        users.add(u);
		    }
		
		    public synchronized void addQuery(String q) {
		        queries.add(q);
		    }
		
		    public synchronized void removeUser(String u) {
		        users.remove(u);
		    }
		
		    public synchronized void removeQuery(String q) {
		        queries.remove(q);
		    }
		}

- 使用分拆的锁重构ServerStatus

		@ThreadSafe
		public class ServerStatusAfterSplit {
		    @GuardedBy("users") public final Set<String> users;
		    @GuardedBy("queries") public final Set<String> queries;
		
		    public ServerStatusAfterSplit() {
		        users = new HashSet<String>();
		        queries = new HashSet<String>();
		    }
		
		    public void addUser(String u) {
		        synchronized (users) {
		            users.add(u);
		        }
		    }
		
		    public void addQuery(String q) {
		        synchronized (queries) {
		            queries.add(q);
		        }
		    }
		
		    public void removeUser(String u) {
		        synchronized (users) {
		            users.remove(u);
		        }
		    }
		
		    public void removeQuery(String q) {
		        synchronized (users) {
		            queries.remove(q);
		        }
		    }
		}

- 分拆锁之后，每个新的更精巧的锁，相比于那些原始的粗糙锁，将会看到更少的通信量。

### 11.4.3 分离锁
- 把一个锁竞争激烈的锁分拆成两个，很可能形成两个竞争激烈的锁。
- 分拆锁有时候可以被扩展，分成可大可小加锁块的集合，并且它们归属于相互独立的对象，这样的情况就是分离锁。例如，ConcurrentHashMap的实现使用了一个包含16个锁的Array，每一个锁都守护Hash Bucket的1 / 16；Bucket N由第N mod 16个锁来守护。假设哈希提供合理的拓展特性，并且关键字能够以统一的方式访问，这将会把对于锁的请求减少到约为原来的1 / 16。这项技术使得ConcurrentHashMap能够支持16个并发的Writer。
- 分离锁的一个负面作用是：对容器加锁，进行独占访问更加困难，并且更加昂贵了。通常，一个操作可以通过获取最多不超过一个锁来进行，但是有个别的情况需要对整个容器加锁，比如当ConcurrentHashMap的值需要被扩展、重排，放入一个更大的Bucket时。这是获取所有分离的锁的最典型的例子。
- 以下代码，StripedMap阐释了基于哈希的Map实现，其中用到了锁分离。他拥有N_LOCKS个锁，每一个守护Bucket的一个子集，大部分方法，比如get，只需要得到一个Bucket锁。有些方法需要获得所有的锁，但是如clear方法实现的那样，并不需要同时获得所有这些锁。

		//基于哈希的map中使用分离锁
		@ThreadSafe
		public class StripedMap {
		    // Synchronization policy: buckets[n] guarded by locks[n%N_LOCKS]
		    private static final int N_LOCKS = 16;
		    private final Node[] buckets;
		    private final Object[] locks;
		
		    private static class Node {
		        Node next;
		        Object key;
		        Object value;
		    }
		
		    public StripedMap(int numBuckets) {
		        buckets = new Node[numBuckets];
		        locks = new Object[N_LOCKS];
		        for (int i = 0; i < N_LOCKS; i++)
		            locks[i] = new Object();
		    }
		
		    private final int hash(Object key) {
		        return Math.abs(key.hashCode() % buckets.length);
		    }
		
		    public Object get(Object key) {
		        int hash = hash(key);
		        synchronized (locks[hash % N_LOCKS]) {
		            for (Node m = buckets[hash]; m != null; m = m.next)
		                if (m.key.equals(key))
		                    return m.value;
		        }
		        return null;
		    }
		
		    public void clear() {
		        for (int i = 0; i < buckets.length; i++) {
		            synchronized (locks[i % N_LOCKS]) {
		                buckets[i] = null;
		            }
		        }
		    }
		}


### 11.4.4 避免热点域
- 分拆锁和分离锁能够改进可伸缩性，因为它们能够使不同线程操作不同的数据或者(相同数据结构的不同部分)，而不会发生相互干扰。能够从分析锁受益的程序，通常是那些对锁的竞争普遍大于对锁守护数据竞争的程序。如果一个锁守护两个独立变量X和Y，线程A想要访问X，而线程B想要访问Y，这两个线程没有竞争任何数据，然而它们竞争相同的锁。
- 独立的计数器可以提高类似size方法的速度，却使改进可伸缩性变得更难，因为每一个修改map的操作都要更新这个共享的计数器。计数器被称为“热点域(hot field)，因为每个变化操作都要访问它。
- 为避免这个问题，ConcurrentHashMap通过枚举每个条目获得size，并把这个值加入到每个条目，而不是维护一个全局计数。为了避免列举所有元素，ConcurrentHashMap为每一个条目维护一个独立的计数域。同样由分离的锁守护。

### 11.4.5 独占锁的替代方法
- 用于减轻竞争锁带来的影响的第三种技术是提前使用独占锁，这有助于使用更友好的并发方式进行共享状态的管理。这包括使用：
	- 并发容器
	- 读写锁
	- 不可变对象
	- 原子变量
- ReadWriteLock实行了一个多读者单写者加锁规则：只要没有更改，那么多个读者可以并发访问共享资源，但是写者必须独占获得锁。、
- 原子变量提供了能够减少更新“热点域”的方式，如静态计数器、序列发生器、或者对链表数据结构头结点的引用。原子变量类提供了针对整数或对象引用的非常精妙的原子操作(因此更具有可伸缩性)，并且使用现代处理器提供的低层并发原语，比如比较并交换实现。

### 11.4.6 检测CPU利用率
- 当我们测试可伸缩性的时候，我们的目标通常是保持处理器的充分利用。Unix系统的vmstat和mpstat，或者Windows的perfmon都可以查看处理器忙碌状况。
- 如果CPU没有完全利用，可能以下原因：
	- 不充足的负载。
	- I/O限制。
	- 外部限制。比如数据库，或者Web Service
	- 锁竞争。使用Profiling工具能够告诉你，程序中存在多少个锁的竞争，哪些锁很抢手。

### 11.4.7 向“对象池”说“不”
- 针对对象的慢生命周期，很多程序员都会选择使用对象池化技术，这项技术中，对象会被循环使用，而不是由垃圾收集器回收并在需要时重新分配。在单线程化的程序中，即使考虑到减少的垃圾收集开销，对象化池技术对于所有不那么昂贵的对象仍然存在性能缺失。
- 在并发中，池化表现的更糟糕。当线程分配新的对象时，需要线程内部非常细微的协调，因为分配运算通常使用线程本地的分配块来消除对象堆中的大部分同步。

## 11.5 比较Map的性能
- 单线程化的ConcurrentHashMap的性能比同步的HashMap的性能稍好一些，而且在并发应用中，这种作用就十分明显了。
- 同步Map的整个Map存在一个锁，所以一次只有一个线程能够访问map
- ConcurrentHashMap并没有对成功的读操作加锁，对写操作和真正需要锁的操作的读操作使用了分离锁的方法。所以多线程并发访问Map，而不被阻塞。

## 11.6 减少上下文切换的开销
- 很多任务引入的操作都会发生阻塞；在运行和阻塞这两个状态之间转换需要使用上下文切换。
- 服务器应用程序发生阻塞的一个缘由是在处理请求期间产生日志消息；
- 并发系统在多数锁为非竞争锁的时候会有更好的性能，因为请求竞争性的锁意味着更多的上下文切换。代码如果造成更多的上下文切换意味着产生更小的吞吐量。


## summary
- 使用线程最主要的目的是利用多处理器资源
- 在并发程序性能讨论中，我们通常更关注吞吐量和可伸缩性，而没有强调自然服务时间。
- Amdahl定律告诉我们，程序的可伸缩性是由必须连续执行的代码比例决定的。
- 因为Java程序中串行化首要的来源是独占的资源锁，所以可伸缩性通常可以通过以下这些方式提升：
	- 减少用于获取锁的时间，
	- 减小锁的粒度
	- 减少锁的占用时间
	- 用非独占或非阻塞锁来取代独占锁
 